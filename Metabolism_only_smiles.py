# -*- coding: utf-8 -*-
"""대사 안정성 예측_Only_SMILES

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XPirlUY4QmbgR-LXp85o9TGkSiwLi3bt

# Model Training

## install and import
"""
import random
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import deepchem as dc
import rdkit
from rdkit import DataStructs
from rdkit import Chem
from rdkit.Chem import AllChem, PandasTools, Descriptors

from sklearn.model_selection import KFold
from deepchem.splits.splitters import ScaffoldSplitter
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression # stacking모델
from tensorflow.keras.models import load_model
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.linear_model import Ridge, Lasso

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers
from keras import models
from deepchem.models.graph_models import GraphConvModel

import joblib
import pickle
import datetime
import pytz

"""## Feature Enginnering"""
# 0D-Descriptor : RDKit Descriptor
def get_descriptor_feature(test_df, min_max_mean = True):

  PandasTools.AddMoleculeColumnToFrame(test_df,'SMILES')
  test_mol = test_df.ROMol
  test_rdk = {}

  for i,j in Descriptors.descList: # RDkit Descrpior 적용
    test_rdk[i] = test_mol.apply(j)

  if min_max_mean:
    try:
      test_rdk['MaxAbsPartialCharge'] = np.where(pd.isna(test_rdk['MaxAbsPartialCharge']), test_rdk['MaxAbsPartialCharge'].mean() ,test_rdk['MaxAbsPartialCharge'])
      test_rdk['MinAbsPartialCharge'] = np.where(pd.isna(test_rdk['MinAbsPartialCharge']), test_rdk['MinAbsPartialCharge'].mean() ,test_rdk['MinAbsPartialCharge'])
    except:
      pass

  test_rdk = pd.DataFrame(test_rdk)

  rdk_list = ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex',
       'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt',
       'NumValenceElectrons', 'MaxAbsPartialCharge', 'MinAbsPartialCharge',
       'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'AvgIpc',
       'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n',
       'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v',
       'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA',
       'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA6',
       'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10',
       'SMR_VSA3', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SlogP_VSA1',
       'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA5', 'SlogP_VSA6', 'TPSA',
       'EState_VSA10', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4',
       'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8',
       'EState_VSA9', 'VSA_EState1', 'VSA_EState2', 'VSA_EState3',
       'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7',
       'VSA_EState8', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount',
       'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings',
       'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds',
       'RingCount', 'MolLogP', 'MolMR', 'fr_Ar_N', 'fr_NH0', 'fr_benzene']
  
  test_rdk = test_rdk.loc[:,rdk_list]

  return test_rdk

# Fingerprint Descriptor
class FpsGenerator:
    def __init__(self, size=2048, radius=4):
        self.featurizer = dc.feat.CircularFingerprint(size=size, radius=radius)

    def generate_fingerprints(self, df):
        list_fps = df['SMILES'].apply(self.featurizer.featurize)
        fps_features = np.vstack(list_fps)
        fps_df = pd.DataFrame(fps_features)
        fps_df.columns = fps_df.columns.astype(str)
        return fps_df

# Mol2Vec Fingerprint
class Mol2Vec:
  def __init__(self):
    self.featurizer = dc.feat.Mol2VecFingerprint()

  def generate_mol2vecDataFrame(self, df):
    list_vec = df['SMILES'].apply(self.featurizer.featurize)
    vec_features = np.vstack(list_vec)
    vec_df = pd.DataFrame(vec_features)
    vec_df.columns = vec_df.columns.astype(str)
    return vec_df

# ConvMolFeaturizer
class ConvMol:
  def __init__(self):
    self.featurizer = dc.feat.ConvMolFeaturizer()

  def generate_Convmoldata(self, df):
    mols = [Chem.MolFromSmiles(s) for s in df.SMILES]
    conv_list = self.featurizer.featurize(mols)
    conv_df = pd.DataFrame({'conv':conv_list})
    return conv_df

"""## Split Dataset / label"""

class DataSplitter:
  def __init__(self,seed=42):
    self.seed = seed

  def create_dataframe(self,df):
    dataset = dc.data.NumpyDataset(X=np.array(df.iloc[:,:3]), ids = df['SMILES'])
    # train / valid dataset split for ScaffoldSplitter
    splitter = dc.splits.ScaffoldSplitter()
    train_dataset, valid_dataset = splitter.train_test_split(dataset, frac_train=0.86, seed=self.seed)
    # train / valid df split
    train_df = df[df['SMILES'].isin(train_dataset.ids)]
    valid_df = df[df['SMILES'].isin(valid_dataset.ids)]
    return train_df, valid_df

def feature_label_split(train_df,valid_df):
  hlm_loc = train_df.columns.get_loc('HLM')
  mlm_loc = train_df.columns.get_loc('MLM')
  train_feature = train_df.iloc[:,hlm_loc+1:]
  train_label = train_df.iloc[:,mlm_loc:hlm_loc+1]

  valid_feature = valid_df.iloc[:,hlm_loc+1:]
  valid_label = valid_df.iloc[:,mlm_loc:hlm_loc+1]
  print(f'train_feature, train_label, valid_feature, valid_label 로 저장됨')
  return train_feature, train_label, valid_feature, valid_label

"""## Modeling"""

def show_reg_result(y_test, y_pred, N=50):
    mae = mean_absolute_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    max_err = np.abs(y_test - y_pred).max()

    print('MAE:', round(mae, 4))
    print('RMSE:', round(rmse,4))
    print('Max error:', round(max_err, 4))

    # 일부 실제값과 예측값 샘플을 plot으로 비교하여 그려본다 (N 개)

    if N > 0:
      plt.figure(figsize=(10, 6))
      plt.plot(y_pred[:N], ".b-", label="prediction", linewidth=1.0)
      plt.plot(y_test[:N], '.r-', label="actual", linewidth=1.0)
      plt.xlim(-1,N+1)
      plt.legend()
      plt.ylabel('MLM/HLM')
      plt.show()

seed = 42
dropout = 0.3

class MyCNNModel(tf.keras.Model): #for vec
    def __init__(self, input_size, num_filters, kernel_size, dropout_rate, out_size):
        super(MyCNNModel, self).__init__()

        # Define the 1D convolutional layers
        self.conv1 = tf.keras.layers.Conv1D(num_filters * 2, kernel_size, activation='relu', padding='same')
        self.conv2 = tf.keras.layers.Conv1D(num_filters * 2, kernel_size, activation='relu', padding='same')

        self.ln1 = tf.keras.layers.LayerNormalization()
        self.ln2 = tf.keras.layers.LayerNormalization()

        self.flatten = tf.keras.layers.Flatten()
        self.dense = tf.keras.layers.Dense(256,activation = 'relu')
        self.fc_out = tf.keras.layers.Dense(out_size)

        self.dropout = tf.keras.layers.Dropout(dropout_rate)

    def call(self, inputs):
        out = self.conv1(inputs)
        out = self.ln1(out)
        out = self.dropout(out)

        out = self.conv2(out)
        out = self.ln2(out)
        out = self.dropout(out)

        out = self.flatten(out)
        out = self.dense(out)
        out = self.fc_out(out)
        return out

def rmse(y_true, y_pred):
    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))

"""## Ensemble

### mean
"""

def model_mean(pred_sum_label_rf,pred_sum_label_xgb,pred_vec_label,pred_conv_label): # 모델 합계 평균 검증용
  mlm_mean = []
  for i in range(len(pred_sum_label_rf)):
    mlm = np.mean([pred_sum_label_rf[i][0],pred_sum_label_xgb[i][0], pred_vec_label[i][0],pred_conv_label[i][0]])
    mlm_mean.append(mlm)

  hlm_mean = []
  for i in range(len(pred_sum_label_rf)):
    hlm = np.mean([pred_sum_label_rf[i][1],pred_sum_label_xgb[i][1],pred_vec_label[i][1],pred_conv_label[i][1]])
    hlm_mean.append(hlm)

  mean_df = pd.DataFrame({'MLM':mlm_mean,'HLM':hlm_mean})
  mean_numpy = mean_df.to_numpy()
  return mean_df


with open('/content/drive/MyDrive/약물대사예측/rf_model.pickle','rb') as file:
  rf_model_sum = pickle.load(file)

with open('/content/drive/MyDrive/약물대사예측/xgb_model.pickle','rb') as file:
  xgb_model_sum = pickle.load(file)

best_vec_model = MyCNNModel(input_size=300, num_filters=32, kernel_size=5, dropout_rate=0.3, out_size=2)
best_vec_model.build(input_shape=(64, 300, 1))
best_vec_model.load_weights('/content/drive/MyDrive/약물대사예측/best_vec_model.h5')

best_vec_model.compile(optimizer='adam',loss='mse',metrics=[rmse])

best_gcn_model = GraphConvModel(n_tasks=2, mode='regression',graph_conv_layers=[64,64],batch_size=64,random_state=seed,
                             dropout_rate=dropout,
                             dense_layer_size= 256,
                             model_dir='/content/drive/MyDrive/약물대사예측/gcn_model')

best_gcn_model.restore() #드디어...

def feature_engineering(test_file):
  test_rdk = get_descriptor_feature(test_file)

  featurizer = FpsGenerator(size=2048,radius=4)
  # test에 대한 Circular Fingerprint 생성
  test_fps_df = featurizer.generate_fingerprints(test_file)

  # FPs 및 rdk는 합쳐주기
  test_sum = pd.concat([test_rdk,test_fps_df],axis=1)

  # Mol2Vec
  vec_featurizer = Mol2Vec()
  test_vec_df = vec_featurizer.generate_mol2vecDataFrame(test_file)

  # test에 대한 conv_list 생성
  conv_featurizer = ConvMol()
  test_conv_df = conv_featurizer.generate_Convmoldata(test_file)
  test_conv = pd.concat([test_file.iloc[:,:1],test_conv_df],axis=1)
  
  return test_sum, test_vec_df, test_conv

def transform_df(test_vec_df,test_conv):
  # if you need transform to numpy
  test_vec_feature_np = test_vec_df.to_numpy()

  # Reshape the input data to match the expected 3D shape
  test_vec_feature_np =  test_vec_feature_np.reshape(test_vec_feature_np.shape[0],test_vec_feature_np.shape[1], 1)

  test_conv_dataset = dc.data.NumpyDataset(X=np.array(test_conv.iloc[:,1]))
  return test_vec_feature_np, test_conv_dataset

def predict(test_sum,test_vec_feature_np,test_conv_dataset):
  test_sum_label_rf = rf_model_sum.predict(test_sum)
  test_sum_label_xgb = xgb_model_sum.predict(test_sum)
  test_vec_label = best_vec_model.predict(test_vec_feature_np)
  test_conv_label = best_gcn_model.predict(test_conv_dataset)
  return test_sum_label_rf, test_sum_label_xgb, test_vec_label,test_conv_label

def SMILES2Metabolicrate(test_file):
  test_sum, test_vec_df, test_conv = feature_engineering(test_file)
  test_vec_feature_np, test_conv_dataset = transform_df(test_vec_df,test_conv)
  test_sum_label_rf,test_sum_label_xgb,test_vec_label,test_conv_label = predict(test_sum,test_vec_feature_np,test_conv_dataset)
  return model_mean(test_sum_label_rf,test_sum_label_xgb,test_vec_label,test_conv_label)




